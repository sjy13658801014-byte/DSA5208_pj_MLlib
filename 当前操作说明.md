### 项目名称与数据存储
- Google Cloud Project ID: dsa5208--project--2
- GCS Bucket 名称: dsa5208--project--2--data
- 数据位置: 原始压缩包 2024.tar.gz 在 Bucket 根目录；解压后的 CSV 在 csv_data/ 文件夹  
  ✋说明：这里我已经上传了整个数据的压缩包，解压好放到了csv_data文件夹中，之后训练的时候可以先读到HDFS（似乎性能更好），也可以从csv直接读取数据。这个上传过程是花了一点钱的🥲，但是只是存在GCS里面似乎不是很花钱，所以就先存着这么用用看吧🥲我设置了预算，花到一定的钱之后可能会给我们发邮件提醒的。

### workflow:
- **step1**: 创建 Dataproc Cluster  
  ✋说明: 这里建议在电脑终端复制这两段代码操作，我之前尝试在dataproc那边直接点击创建，换了好多地区和不同类型的vm都失败了，不知道为什么😅
1. 指定项目名称
```
gcloud config set project dsa5208--project--2
```
2. 创建cluster（1主节点 + 2工作节点 / n4-4核）（这个配置可修改）
```
gcloud dataproc clusters create cluster-spark \
    --region us-central1 \
    --image-version 2.2-debian12 \
    --master-machine-type n4-standard-4 \
    --master-boot-disk-size 100 \
    --num-workers 2 \
    --worker-machine-type n4-standard-4 \
    --worker-boot-disk-size 100 \
    --optional-components JUPYTER \
    --enable-component-gateway
```
- **step 2**: 更新/获取代码（在本地电脑）  
  ❗️更新：之前打算在vm的终端拉取代码，但是实践发现创建的集群默认是不联网的，无法用git，而且创建集群之后边改代码边跑确实比较花钱，所以改为采用本地修改代码，github同步，再上传到google cloud跑。
  首次拉取
  ```
  git clone https://github.com/sjy13658801014-byte/DSA5208_pj_2.git
  ```
  后续
  ```
  cd DSA5208_pj_2
  ```
  ```
  git pull
  ```
- **step 3**: （不做这一步）数据上料到 HDFS (在 JupyterLab 终端)
  ✋说明：这里是一个一个把解压后的文件读过来，会花一些时间，不确定这个是不是很花钱的操作🥲我们可以观察一下
  ❗️更新：研究了一下这一步比较花钱，不需要采用。
  ```
  hadoop distcp gs://dsa5208--project--2--data/csv_data/ /project_data
  ```
- **step 4**: 跑完之后一定记得删除集群，最好把代码下载一下
